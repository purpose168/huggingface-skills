#!/usr/bin/env python3
# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "trl>=0.12.0",
#     "peft>=0.7.0",
#     "transformers>=4.36.0",
#     "accelerate>=0.24.0",
#     "trackio",
# ]
# ///

"""
ç”Ÿäº§å°±ç»ªçš„ SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰è®­ç»ƒç¤ºä¾‹ï¼ŒåŒ…å«æ‰€æœ‰æœ€ä½³å®è·µã€‚

æœ¬è„šæœ¬æ¼”ç¤ºäº†ï¼š
- Trackio é›†æˆï¼Œç”¨äºå®æ—¶ç›‘æ§
- LoRA/PEFT ç”¨äºé«˜æ•ˆè®­ç»ƒ
- æ­£ç¡®çš„ Hub ä¿å­˜é…ç½®
- è®­ç»ƒ/è¯„ä¼°åˆ†å‰²ï¼Œç”¨äºç›‘æ§
- æ£€æŸ¥ç‚¹ç®¡ç†
- ä¼˜åŒ–çš„è®­ç»ƒå‚æ•°

ä½¿ç”¨ hf_jobs MCP å·¥å…·çš„ç”¨æ³•ï¼š
    hf_jobs("uv", {
        "script": '''<ç²˜è´´æ•´ä¸ªæ–‡ä»¶å†…å®¹>''',
        "flavor": "a10g-large",
        "timeout": "3h",
        "secrets": {"HF_TOKEN": "$HF_TOKEN"},
    })

æˆ–è€…ç›´æ¥å†…è”æäº¤è„šæœ¬å†…å®¹ï¼Œæ— éœ€ä¿å­˜åˆ°æ–‡ä»¶ã€‚
"""

import trackio
from datasets import load_dataset
from peft import LoraConfig
from trl import SFTTrainer, SFTConfig


# åŠ è½½æ•°æ®é›†
print("ğŸ“¦ æ­£åœ¨åŠ è½½æ•°æ®é›†...")
dataset = load_dataset("trl-lib/Capybara", split="train")
print(f"âœ… æ•°æ®é›†å·²åŠ è½½ï¼š{len(dataset)} ä¸ªç¤ºä¾‹")

# åˆ›å»ºè®­ç»ƒ/è¯„ä¼°åˆ†å‰²
print("ğŸ”€ æ­£åœ¨åˆ›å»ºè®­ç»ƒ/è¯„ä¼°åˆ†å‰²...")
dataset_split = dataset.train_test_split(test_size=0.1, seed=42)
train_dataset = dataset_split["train"]
eval_dataset = dataset_split["test"]
print(f"   è®­ç»ƒé›†ï¼š{len(train_dataset)} ä¸ªç¤ºä¾‹")
print(f"   è¯„ä¼°é›†ï¼š{len(eval_dataset)} ä¸ªç¤ºä¾‹")

# æ³¨æ„ï¼šå¯¹äºå†…å­˜å—é™çš„æ¼”ç¤ºï¼Œå¯ä»¥è·³è¿‡è¯„ä¼°ï¼Œå°†å®Œæ•´æ•°æ®é›†ç”¨ä½œ train_dataset
# å¹¶ä»ä¸‹é¢çš„é…ç½®ä¸­ç§»é™¤ eval_datasetã€eval_strategy å’Œ eval_steps

# è®­ç»ƒé…ç½®
config = SFTConfig(
    # å…³é”®è®¾ç½®ï¼šHub é…ç½®
    output_dir="qwen-capybara-sft",  # è¾“å‡ºç›®å½•åç§°
    push_to_hub=True,  # æ˜¯å¦æ¨é€åˆ° Hugging Face Hub
    hub_model_id="username/qwen-capybara-sft",  # Hub ä¸Šçš„æ¨¡å‹ IDï¼ˆéœ€è¦æ›¿æ¢ä¸ºæ‚¨çš„ç”¨æˆ·åï¼‰
    hub_strategy="every_save",  # æ¨é€ç­–ç•¥ï¼šæ¯æ¬¡ä¿å­˜æ—¶éƒ½æ¨é€æ£€æŸ¥ç‚¹

    # è®­ç»ƒå‚æ•°
    num_train_epochs=3,  # è®­ç»ƒè½®æ•°
    per_device_train_batch_size=4,  # æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒæ‰¹æ¬¡å¤§å°
    gradient_accumulation_steps=4,  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæœ‰æ•ˆæ‰¹æ¬¡å¤§å° = per_device_train_batch_size * gradient_accumulation_stepsï¼‰
    learning_rate=2e-5,  # å­¦ä¹ ç‡
    # max_length=1024,  # æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤å€¼ - ä»…åœ¨éœ€è¦ä¸åŒçš„åºåˆ—é•¿åº¦æ—¶è®¾ç½®ï¼‰

    # æ—¥å¿—è®°å½•å’Œæ£€æŸ¥ç‚¹ä¿å­˜
    logging_steps=10,  # æ¯éš”å¤šå°‘æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
    save_strategy="steps",  # ä¿å­˜ç­–ç•¥ï¼šæŒ‰æ­¥æ•°ä¿å­˜
    save_steps=100,  # æ¯éš”å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
    save_total_limit=2,  # æœ€å¤šä¿ç•™çš„æ£€æŸ¥ç‚¹æ•°é‡

    # è¯„ä¼° - é‡è¦ï¼šä»…å½“æä¾›äº† eval_dataset æ—¶æ‰å¯ç”¨
    eval_strategy="steps",  # è¯„ä¼°ç­–ç•¥ï¼šæŒ‰æ­¥æ•°è¯„ä¼°
    eval_steps=100,  # æ¯éš”å¤šå°‘æ­¥è¯„ä¼°ä¸€æ¬¡

    # ä¼˜åŒ–è®¾ç½®
    warmup_ratio=0.1,  # é¢„çƒ­æ¯”ä¾‹ï¼ˆæ€»è®­ç»ƒæ­¥æ•°çš„ 10% ç”¨äºå­¦ä¹ ç‡é¢„çƒ­ï¼‰
    lr_scheduler_type="cosine",  # å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ï¼šä½™å¼¦é€€ç«

    # ç›‘æ§è®¾ç½®
    report_to="trackio",  # é›†æˆ Trackio è¿›è¡Œç›‘æ§
    project="meaningful_project_name",  # é¡¹ç›®åç§°ï¼ˆç”¨äº Trackioï¼Œå»ºè®®ä½¿ç”¨æœ‰æ„ä¹‰çš„åç§°ï¼‰
    run_name="baseline-run",  # æœ¬æ¬¡è®­ç»ƒè¿è¡Œçš„æè¿°æ€§åç§°
)

# LoRAï¼ˆä½ç§©è‡ªé€‚åº”ï¼‰é…ç½®
# LoRA æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡æ·»åŠ ä½ç§©çŸ©é˜µæ¥å‡å°‘å¯è®­ç»ƒå‚æ•°æ•°é‡
peft_config = LoraConfig(
    r=16,  # LoRA ç§©ï¼ˆä½ç§©çŸ©é˜µçš„ç»´åº¦ï¼‰ï¼Œå€¼è¶Šå¤§å¯è®­ç»ƒå‚æ•°è¶Šå¤šï¼Œä½†æ˜¾å­˜å ç”¨ä¹Ÿè¶Šå¤§
    lora_alpha=32,  # LoRA ç¼©æ”¾å› å­ï¼ˆé€šå¸¸è®¾ç½®ä¸º r çš„ 2 å€ï¼‰ï¼Œç”¨äºæ§åˆ¶ LoRA æ›´æ–°çš„æƒé‡
    lora_dropout=0.05,  # LoRA å±‚çš„ dropout æ¦‚ç‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ
    bias="none",  # åç½®é¡¹çš„è®­ç»ƒæ–¹å¼ï¼š"none"ï¼ˆä¸è®­ç»ƒï¼‰ã€"all"ï¼ˆå…¨éƒ¨è®­ç»ƒï¼‰ã€"lora_only"ï¼ˆä»…è®­ç»ƒ LoRA å±‚çš„åç½®ï¼‰
    task_type="CAUSAL_LM",  # ä»»åŠ¡ç±»å‹ï¼šå› æœè¯­è¨€æ¨¡å‹ï¼ˆç”¨äºç”Ÿæˆå¼æ¨¡å‹ï¼‰
    target_modules=["q_proj", "v_proj"],  # è¦åº”ç”¨ LoRA çš„ç›®æ ‡æ¨¡å—ï¼ˆé€šå¸¸é€‰æ‹©æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æŸ¥è¯¢å’Œå€¼æŠ•å½±å±‚ï¼‰
)

# åˆå§‹åŒ–è®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
print("ğŸ¯ æ­£åœ¨åˆå§‹åŒ–è®­ç»ƒå™¨...")
trainer = SFTTrainer(
    model="Qwen/Qwen2.5-0.5B",  # åŸºç¡€æ¨¡å‹ï¼šQwen 2.5 0.5B å‚æ•°ç‰ˆæœ¬
    train_dataset=train_dataset,  # è®­ç»ƒæ•°æ®é›†
    eval_dataset=eval_dataset,  # è¯„ä¼°æ•°æ®é›†ï¼ˆå…³é”®ï¼šå½“å¯ç”¨ eval_strategy æ—¶å¿…é¡»æä¾›ï¼‰
    args=config,  # è®­ç»ƒé…ç½®
    peft_config=peft_config,  # LoRA/PEFT é…ç½®
)

print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
trainer.train()

print("ğŸ’¾ æ­£åœ¨æ¨é€åˆ° Hub...")
trainer.push_to_hub()

# å®Œæˆ Trackio è·Ÿè¸ª
trackio.finish()

print("âœ… å®Œæˆï¼æ¨¡å‹ä½äºï¼šhttps://huggingface.co/username/qwen-capybara-sft")
print("ğŸ“Š æŸ¥çœ‹æŒ‡æ ‡ï¼šhttps://huggingface.co/spaces/username/trackio")
