#!/usr/bin/env python3
# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "trl>=0.12.0",
#     "transformers>=4.36.0",
#     "accelerate>=0.24.0",
#     "trackio",
# ]
# ///

"""
ç”Ÿäº§çº§ DPO è®­ç»ƒç¤ºä¾‹ï¼Œç”¨äºåå¥½å­¦ä¹ ã€‚

DPOï¼ˆç›´æ¥åå¥½ä¼˜åŒ–ï¼‰åœ¨åå¥½å¯¹ï¼ˆchosen vs rejected responsesï¼Œå³é€‰ä¸­ä¸æ‹’ç»çš„å“åº”ï¼‰ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œ
æ— éœ€å¥–åŠ±æ¨¡å‹ï¼ˆreward modelï¼‰ã€‚

ä½¿ç”¨ hf_jobs MCP å·¥å…·çš„ç”¨æ³•ï¼š
    hf_jobs("uv", {
        "script": '''<ç²˜è´´æ•´ä¸ªæ–‡ä»¶å†…å®¹>''',
        "flavor": "a10g-large",
        "timeout": "3h",
        "secrets": {"HF_TOKEN": "$HF_TOKEN"},
    })

æˆ–è€…ç›´æ¥å†…è”æäº¤è„šæœ¬å†…å®¹ï¼Œæ— éœ€ä¿å­˜åˆ°æ–‡ä»¶ã€‚
"""

import trackio
from datasets import load_dataset
from trl import DPOTrainer, DPOConfig


# åŠ è½½åå¥½æ•°æ®é›†
# åå¥½æ•°æ®é›†åŒ…å«æˆå¯¹çš„æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«ä¸€ä¸ª"é€‰ä¸­"ï¼ˆchosenï¼‰å“åº”å’Œä¸€ä¸ª"æ‹’ç»"ï¼ˆrejectedï¼‰å“åº”
print("ğŸ“¦ æ­£åœ¨åŠ è½½æ•°æ®é›†...")
dataset = load_dataset("trl-lib/ultrafeedback_binarized", split="train")
print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼š{len(dataset)} ä¸ªåå¥½å¯¹")

# åˆ›å»ºè®­ç»ƒé›†/éªŒè¯é›†åˆ’åˆ†
# å°†æ•°æ®é›†æŒ‰ 90:10 çš„æ¯”ä¾‹åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œseed=42 ç¡®ä¿å¯é‡å¤æ€§
print("ğŸ”€ æ­£åœ¨åˆ›å»ºè®­ç»ƒé›†/éªŒè¯é›†åˆ’åˆ†...")
dataset_split = dataset.train_test_split(test_size=0.1, seed=42)
train_dataset = dataset_split["train"]  # è®­ç»ƒé›†ï¼ŒåŒ…å« 90% çš„æ•°æ®
eval_dataset = dataset_split["test"]   # éªŒè¯é›†ï¼ŒåŒ…å« 10% çš„æ•°æ®
print(f"   è®­ç»ƒé›†ï¼š{len(train_dataset)} ä¸ªåå¥½å¯¹")
print(f"   éªŒè¯é›†ï¼š{len(eval_dataset)} ä¸ªåå¥½å¯¹")

# è®­ç»ƒé…ç½®
# DPOConfig åŒ…å«äº† DPO è®­ç»ƒæ‰€éœ€çš„æ‰€æœ‰è¶…å‚æ•°å’Œè®¾ç½®
config = DPOConfig(
    # å…³é”®è®¾ç½®ï¼šHub é…ç½®
    # output_dir: æ¨¡å‹è¾“å‡ºç›®å½•ï¼Œç”¨äºä¿å­˜è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ£€æŸ¥ç‚¹å’Œæœ€ç»ˆæ¨¡å‹
    output_dir="qwen-dpo-aligned",
    # push_to_hub: æ˜¯å¦å°†æ¨¡å‹æ¨é€åˆ° Hugging Face Hub
    push_to_hub=True,
    # hub_model_id: Hub ä¸Šçš„æ¨¡å‹ IDï¼Œæ ¼å¼ä¸º "username/model-name"
    hub_model_id="username/qwen-dpo-aligned",
    # hub_strategy: æ¨é€ç­–ç•¥ï¼Œ"every_save" è¡¨ç¤ºæ¯æ¬¡ä¿å­˜æ£€æŸ¥ç‚¹æ—¶éƒ½æ¨é€åˆ° Hub
    hub_strategy="every_save",

    # DPO ç‰¹å®šå‚æ•°
    # beta: KL æ•£åº¦æƒ©ç½šç³»æ•°ï¼Œæ§åˆ¶æ¨¡å‹ä¸å‚è€ƒæ¨¡å‹çš„åç¦»ç¨‹åº¦
    # è¾ƒé«˜çš„ beta å€¼ä¼šä½¿æ¨¡å‹æ›´æ¥è¿‘å‚è€ƒæ¨¡å‹ï¼Œè¾ƒä½çš„ beta å€¼å…è®¸æ›´å¤§çš„åç¦»
    beta=0.1,  # KL æƒ©ç½šç³»æ•°ï¼ˆå€¼è¶Šé«˜ = è¶Šæ¥è¿‘å‚è€ƒæ¨¡å‹ï¼‰

    # è®­ç»ƒå‚æ•°
    # num_train_epochs: è®­ç»ƒè½®æ•°ï¼ŒDPO é€šå¸¸éœ€è¦çš„è½®æ•°æ¯” SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰å°‘
    num_train_epochs=1,  # DPO é€šå¸¸éœ€è¦çš„è½®æ•°æ¯” SFT å°‘
    # per_device_train_batch_size: æ¯ä¸ªè®¾å¤‡ï¼ˆGPU/CPUï¼‰ä¸Šçš„è®­ç»ƒæ‰¹æ¬¡å¤§å°
    per_device_train_batch_size=4,
    # gradient_accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œç”¨äºæ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹æ¬¡å¤§å°
    # å®é™…æ‰¹æ¬¡å¤§å° = per_device_train_batch_size * gradient_accumulation_steps * num_devices
    gradient_accumulation_steps=4,
    # learning_rate: å­¦ä¹ ç‡ï¼ŒDPO ä½¿ç”¨çš„å­¦ä¹ ç‡é€šå¸¸æ¯” SFT ä½å¾—å¤š
    learning_rate=5e-7,  # DPO ä½¿ç”¨çš„å­¦ä¹ ç‡æ¯” SFT ä½å¾—å¤š
    # max_length=1024,  # é»˜è®¤å€¼ - ä»…åœ¨éœ€è¦ä¸åŒçš„åºåˆ—é•¿åº¦æ—¶è®¾ç½®

    # æ—¥å¿—è®°å½•å’Œæ£€æŸ¥ç‚¹ä¿å­˜
    # logging_steps: æ¯éš”å¤šå°‘æ­¥è®°å½•ä¸€æ¬¡è®­ç»ƒæ—¥å¿—
    logging_steps=10,
    # save_strategy: ä¿å­˜ç­–ç•¥ï¼Œ"steps" è¡¨ç¤ºæŒ‰æ­¥æ•°ä¿å­˜
    save_strategy="steps",
    # save_steps: æ¯éš”å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
    save_steps=100,
    # save_total_limit: æœ€å¤šä¿ç•™å¤šå°‘ä¸ªæ£€æŸ¥ç‚¹ï¼Œæ—§çš„ä¼šè¢«åˆ é™¤
    save_total_limit=2,

    # è¯„ä¼° - é‡è¦æç¤ºï¼šä»…åœ¨æä¾› eval_dataset æ—¶å¯ç”¨
    # eval_strategy: è¯„ä¼°ç­–ç•¥ï¼Œ"steps" è¡¨ç¤ºæŒ‰æ­¥æ•°è¿›è¡Œè¯„ä¼°
    eval_strategy="steps",
    # eval_steps: æ¯éš”å¤šå°‘æ­¥è¿›è¡Œä¸€æ¬¡è¯„ä¼°
    eval_steps=100,

    # ä¼˜åŒ–å™¨è®¾ç½®
    # warmup_ratio: å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹ï¼Œåœ¨å‰ warmup_ratio * total_steps æ­¥ä¸­çº¿æ€§å¢åŠ å­¦ä¹ ç‡
    warmup_ratio=0.1,
    # lr_scheduler_type: å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ï¼Œ"cosine" è¡¨ç¤ºä½™å¼¦é€€ç«è°ƒåº¦å™¨
    lr_scheduler_type="cosine",

    # ç›‘æ§å’Œæ—¥å¿—
    # report_to: æŠ¥å‘Šç›®æ ‡ï¼Œ"trackio" è¡¨ç¤ºå°†è®­ç»ƒæŒ‡æ ‡æŠ¥å‘Šåˆ° Trackio å¹³å°
    report_to="trackio",  # ä¸ Trackio é›†æˆ
    # project: é¡¹ç›®åç§°ï¼Œç”¨äºåœ¨ Trackio ä¸­ç»„ç»‡è®­ç»ƒä»»åŠ¡
    project="meaningful_project_name", # è®­ç»ƒçš„é¡¹ç›®åç§°ï¼ˆtrackioï¼‰
    # run_name: è¿è¡Œåç§°ï¼Œç”¨äºæ ‡è¯†è¿™æ¬¡ç‰¹å®šçš„è®­ç»ƒè¿è¡Œ
    run_name="baseline-run", # è¿™æ¬¡è®­ç»ƒè¿è¡Œçš„æè¿°æ€§åç§°

)

# åˆå§‹åŒ–å¹¶å¼€å§‹è®­ç»ƒ
# æ³¨æ„ï¼šDPO éœ€è¦ä¸€ä¸ªç»è¿‡æŒ‡ä»¤å¾®è°ƒï¼ˆinstruct-tunedï¼‰çš„æ¨¡å‹ä½œä¸ºåŸºç¡€æ¨¡å‹
# åŸºç¡€æ¨¡å‹åº”è¯¥å·²ç»ç†è§£å¦‚ä½•éµå¾ªæŒ‡ä»¤ï¼ŒDPO å°†è¿›ä¸€æ­¥ä¼˜åŒ–å…¶å“åº”è´¨é‡
print("ğŸ¯ æ­£åœ¨åˆå§‹åŒ–è®­ç»ƒå™¨...")
trainer = DPOTrainer(
    # model: åŸºç¡€æ¨¡å‹è·¯å¾„ï¼Œå¿…é¡»ä½¿ç”¨ instruct æ¨¡å‹ï¼Œè€Œä¸æ˜¯ base æ¨¡å‹
    # instruct æ¨¡å‹å·²ç»è¿‡æŒ‡ä»¤å¾®è°ƒï¼Œèƒ½å¤Ÿç†è§£å¹¶éµå¾ªæŒ‡ä»¤
    model="Qwen/Qwen2.5-0.5B-Instruct",  # ä½¿ç”¨ instruct æ¨¡å‹ï¼Œè€Œä¸æ˜¯ base æ¨¡å‹
    # train_dataset: è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…å«åå¥½å¯¹
    train_dataset=train_dataset,
    # eval_dataset: éªŒè¯æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½
    # å…³é”®æç¤ºï¼šå½“å¯ç”¨ eval_strategy æ—¶å¿…é¡»æä¾› eval_dataset
    eval_dataset=eval_dataset,  # å…³é”®æç¤ºï¼šå¯ç”¨ eval_strategy æ—¶å¿…é¡»æä¾› eval_dataset
    # args: è®­ç»ƒé…ç½®å¯¹è±¡
    args=config,
)

print("ğŸš€ æ­£åœ¨å¯åŠ¨ DPO è®­ç»ƒ...")
# å¼€å§‹è®­ç»ƒè¿‡ç¨‹
trainer.train()

print("ğŸ’¾ æ­£åœ¨æ¨é€åˆ° Hub...")
# å°†è®­ç»ƒå¥½çš„æ¨¡å‹æ¨é€åˆ° Hugging Face Hub
trainer.push_to_hub()

# å®Œæˆ Trackio è·Ÿè¸ª
# ç»“æŸ Trackio çš„å®éªŒè·Ÿè¸ªï¼Œç¡®ä¿æ‰€æœ‰æŒ‡æ ‡å’Œæ—¥å¿—éƒ½å·²ä¿å­˜
trackio.finish()

print("âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹åœ°å€ï¼šhttps://huggingface.co/username/qwen-dpo-aligned")
print("ğŸ“Š æŸ¥çœ‹è®­ç»ƒæŒ‡æ ‡ï¼šhttps://huggingface.co/spaces/username/trackio")
