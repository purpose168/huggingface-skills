#!/usr/bin/env python3
# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "transformers>=4.36.0",
#     "peft>=0.7.0",
#     "torch>=2.0.0",
#     "accelerate>=0.24.0",
#     "huggingface_hub>=0.20.0",
#     "sentencepiece>=0.1.99",
#     "protobuf>=3.20.0",
#     "numpy",
#     "gguf",
# ]
# ///

"""
GGUF è½¬æ¢è„šæœ¬ - ç”Ÿäº§å°±ç»ªç‰ˆæœ¬

æœ¬è„šæœ¬å°† LoRA å¾®è°ƒæ¨¡å‹è½¬æ¢ä¸º GGUF æ ¼å¼ï¼Œä»¥ä¾¿åœ¨ä»¥ä¸‹å·¥å…·ä¸­ä½¿ç”¨ï¼š
- llama.cpp
- Ollama
- LM Studio
- å…¶ä»–å…¼å®¹ GGUF çš„å·¥å…·

å‰ç½®æ¡ä»¶ï¼ˆè¯·å…ˆå®‰è£…è¿™äº›ï¼‰ï¼š
- Ubuntu/Debian: sudo apt-get update && sudo apt-get install -y build-essential cmake
- RHEL/CentOS: sudo yum groupinstall -y "Development Tools" && sudo yum install -y cmake
- macOS: xcode-select --install && brew install cmake

ä½¿ç”¨æ–¹æ³•ï¼š
    è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
    - ADAPTER_MODEL: æ‚¨çš„å¾®è°ƒæ¨¡å‹ï¼ˆä¾‹å¦‚ï¼š"username/my-finetuned-model"ï¼‰
    - BASE_MODEL: ç”¨äºå¾®è°ƒçš„åŸºç¡€æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼š"Qwen/Qwen2.5-0.5B"ï¼‰
    - OUTPUT_REPO: ä¸Šä¼  GGUF æ–‡ä»¶çš„ä½ç½®ï¼ˆä¾‹å¦‚ï¼š"username/my-model-gguf"ï¼‰
    - HF_USERNAME: æ‚¨çš„ Hugging Face ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œç”¨äº READMEï¼‰

ä¾èµ–é¡¹ï¼šæ‰€æœ‰å¿…éœ€çš„åŒ…å·²åœ¨ä¸Šè¿° PEP 723 å¤´éƒ¨å£°æ˜ã€‚
"""

# å¯¼å…¥å¿…è¦çš„ Python æ ‡å‡†åº“å’Œç¬¬ä¸‰æ–¹åº“
import os  # æ“ä½œç³»ç»Ÿæ¥å£ï¼Œç”¨äºç¯å¢ƒå˜é‡å’Œæ–‡ä»¶æ“ä½œ
import sys  # ç³»ç»Ÿç›¸å…³çš„å‚æ•°å’Œå‡½æ•°
import torch  # PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
from transformers import AutoModelForCausalLM, AutoTokenizer  # Hugging Face Transformers æ¨¡å‹åŠ è½½å™¨
from peft import PeftModel  # PEFTï¼ˆå‚æ•°é«˜æ•ˆå¾®è°ƒï¼‰åº“ï¼Œç”¨äºåŠ è½½ LoRA é€‚é…å™¨
from huggingface_hub import HfApi  # Hugging Face Hub APIï¼Œç”¨äºä¸Šä¼ æ¨¡å‹
import subprocess  # å­è¿›ç¨‹ç®¡ç†ï¼Œç”¨äºæ‰§è¡Œç³»ç»Ÿå‘½ä»¤


def check_system_dependencies():
    """
    æ£€æŸ¥å¿…éœ€çš„ç³»ç»ŸåŒ…æ˜¯å¦å¯ç”¨

    Returns:
        bool: å¦‚æœæ‰€æœ‰ä¾èµ–éƒ½å­˜åœ¨åˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
    """
    print("ğŸ” æ­£åœ¨æ£€æŸ¥ç³»ç»Ÿä¾èµ–...")

    # æ£€æŸ¥ git æ˜¯å¦å·²å®‰è£…
    # subprocess.run ç”¨äºæ‰§è¡Œç³»ç»Ÿå‘½ä»¤ï¼Œcapture_output=True æ•è·è¾“å‡º
    if subprocess.run(["which", "git"], capture_output=True).returncode != 0:
        print("  âŒ git æœªå®‰è£…ã€‚è¯·å®‰è£…å®ƒï¼š")
        print("     Ubuntu/Debian: sudo apt-get install git")
        print("     RHEL/CentOS: sudo yum install git")
        print("     macOS: brew install git")
        return False

    # æ£€æŸ¥ make æˆ– cmake æ˜¯å¦å·²å®‰è£…
    # returncode == 0 è¡¨ç¤ºå‘½ä»¤æ‰§è¡ŒæˆåŠŸ
    has_make = subprocess.run(["which", "make"], capture_output=True).returncode == 0
    has_cmake = subprocess.run(["which", "cmake"], capture_output=True).returncode == 0

    if not has_make and not has_cmake:
        print("  âŒ æœªæ‰¾åˆ° make æˆ– cmakeã€‚è¯·å®‰è£…æ„å»ºå·¥å…·ï¼š")
        print("     Ubuntu/Debian: sudo apt-get install build-essential cmake")
        print("     RHEL/CentOS: sudo yum groupinstall 'Development Tools' && sudo yum install cmake")
        print("     macOS: xcode-select --install && brew install cmake")
        return False

    print("  âœ… ç³»ç»Ÿä¾èµ–å·²æ‰¾åˆ°")
    return True


def run_command(cmd, description):
    """
    æ‰§è¡Œå‘½ä»¤å¹¶è¿›è¡Œé”™è¯¯å¤„ç†

    Args:
        cmd (list): è¦æ‰§è¡Œçš„å‘½ä»¤åŠå…¶å‚æ•°åˆ—è¡¨
        description (str): å‘½ä»¤çš„æè¿°ä¿¡æ¯

    Returns:
        bool: å‘½ä»¤æ‰§è¡ŒæˆåŠŸè¿”å› Trueï¼Œå¤±è´¥è¿”å› False
    """
    print(f"   {description}...")
    try:
        # subprocess.run æ‰§è¡Œå‘½ä»¤ï¼Œcheck=True ä¼šåœ¨å‘½ä»¤å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        # capture_output=True æ•è·æ ‡å‡†è¾“å‡ºå’Œé”™è¯¯è¾“å‡º
        # text=True å°†è¾“å‡ºä½œä¸ºå­—ç¬¦ä¸²è€Œéå­—èŠ‚è¿”å›
        result = subprocess.run(
            cmd,
            check=True,
            capture_output=True,
            text=True
        )
        # å¦‚æœæœ‰æ ‡å‡†è¾“å‡ºï¼Œæ˜¾ç¤ºå‰ 200 ä¸ªå­—ç¬¦
        if result.stdout:
            print(f"   {result.stdout[:200]}")  # æ˜¾ç¤ºå‰ 200 ä¸ªå­—ç¬¦
        return True
    except subprocess.CalledProcessError as e:
        # å¤„ç†å‘½ä»¤æ‰§è¡Œå¤±è´¥çš„æƒ…å†µ
        print(f"   âŒ å‘½ä»¤å¤±è´¥: {' '.join(cmd)}")
        if e.stdout:
            print(f"   æ ‡å‡†è¾“å‡º: {e.stdout[:500]}")
        if e.stderr:
            print(f"   æ ‡å‡†é”™è¯¯: {e.stderr[:500]}")
        return False
    except FileNotFoundError:
        # å¤„ç†å‘½ä»¤æœªæ‰¾åˆ°çš„æƒ…å†µ
        print(f"   âŒ æœªæ‰¾åˆ°å‘½ä»¤: {cmd[0]}")
        return False


# æ‰“å°è„šæœ¬æ ‡é¢˜
print("ğŸ”„ GGUF è½¬æ¢è„šæœ¬")
print("=" * 60)

# é¦–å…ˆæ£€æŸ¥ç³»ç»Ÿä¾èµ–
if not check_system_dependencies():
    print("\nâŒ è¯·å®‰è£…ç¼ºå¤±çš„ç³»ç»Ÿä¾èµ–åé‡è¯•ã€‚")
    sys.exit(1)

# ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
# os.environ.get() è·å–ç¯å¢ƒå˜é‡ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯é»˜è®¤å€¼
ADAPTER_MODEL = os.environ.get("ADAPTER_MODEL", "evalstate/qwen-capybara-medium")  # LoRA é€‚é…å™¨æ¨¡å‹
BASE_MODEL = os.environ.get("BASE_MODEL", "Qwen/Qwen2.5-0.5B")  # åŸºç¡€æ¨¡å‹
OUTPUT_REPO = os.environ.get("OUTPUT_REPO", "evalstate/qwen-capybara-medium-gguf")  # è¾“å‡ºä»“åº“
username = os.environ.get("HF_USERNAME", ADAPTER_MODEL.split('/')[0])  # Hugging Face ç”¨æˆ·å

print(f"\nğŸ“¦ é…ç½®ä¿¡æ¯:")
print(f"   åŸºç¡€æ¨¡å‹: {BASE_MODEL}")
print(f"   é€‚é…å™¨æ¨¡å‹: {ADAPTER_MODEL}")
print(f"   è¾“å‡ºä»“åº“: {OUTPUT_REPO}")

# æ­¥éª¤ 1: åŠ è½½åŸºç¡€æ¨¡å‹å’Œé€‚é…å™¨
print("\nğŸ”§ æ­¥éª¤ 1: æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹å’Œ LoRA é€‚é…å™¨...")
print("   (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)")

try:
    # åŠ è½½åŸºç¡€å› æœè¯­è¨€æ¨¡å‹
    # dtype=torch.float16 ä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•°ä»¥å‡å°‘å†…å­˜å ç”¨
    # device_map="auto" è‡ªåŠ¨å°†æ¨¡å‹åˆ†é…åˆ°å¯ç”¨çš„è®¾å¤‡ï¼ˆCPU/GPUï¼‰
    # trust_remote_code=True å…è®¸æ‰§è¡Œæ¨¡å‹ä»“åº“ä¸­çš„è‡ªå®šä¹‰ä»£ç 
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
    )
    print("   âœ… åŸºç¡€æ¨¡å‹å·²åŠ è½½")
except Exception as e:
    print(f"   âŒ åŠ è½½åŸºç¡€æ¨¡å‹å¤±è´¥: {e}")
    sys.exit(1)

try:
    # åŠ è½½å¹¶åˆå¹¶é€‚é…å™¨
    print("   æ­£åœ¨åŠ è½½ LoRA é€‚é…å™¨...")
    # PeftModel.from_pretrained åŠ è½½ LoRA é€‚é…å™¨å¹¶åº”ç”¨åˆ°åŸºç¡€æ¨¡å‹
    model = PeftModel.from_pretrained(base_model, ADAPTER_MODEL)
    print("   âœ… é€‚é…å™¨å·²åŠ è½½")

    print("   æ­£åœ¨å°†é€‚é…å™¨ä¸åŸºç¡€æ¨¡å‹åˆå¹¶...")
    # merge_and_unload() å°† LoRA æƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­å¹¶å¸è½½é€‚é…å™¨
    merged_model = model.merge_and_unload()
    print("   âœ… æ¨¡å‹å·²åˆå¹¶ï¼")
except Exception as e:
    print(f"   âŒ åˆå¹¶æ¨¡å‹å¤±è´¥: {e}")
    sys.exit(1)

try:
    # åŠ è½½åˆ†è¯å™¨ï¼ˆtokenizerï¼‰
    # åˆ†è¯å™¨ç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ç†è§£çš„ token åºåˆ—
    tokenizer = AutoTokenizer.from_pretrained(ADAPTER_MODEL, trust_remote_code=True)
    print("   âœ… åˆ†è¯å™¨å·²åŠ è½½")
except Exception as e:
    print(f"   âŒ åŠ è½½åˆ†è¯å™¨å¤±è´¥: {e}")
    sys.exit(1)

# æ­¥éª¤ 2: ä¸´æ—¶ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
print("\nğŸ’¾ æ­¥éª¤ 2: æ­£åœ¨ä¿å­˜åˆå¹¶åçš„æ¨¡å‹...")
merged_dir = "/tmp/merged_model"  # ä¸´æ—¶ä¿å­˜ç›®å½•
try:
    # save_pretrained ä¿å­˜æ¨¡å‹æƒé‡å’Œé…ç½®
    # safe_serialization=True ä½¿ç”¨å®‰å…¨åºåˆ—åŒ–æ ¼å¼ï¼ˆsafetensorsï¼‰
    merged_model.save_pretrained(merged_dir, safe_serialization=True)
    tokenizer.save_pretrained(merged_dir)
    print(f"   âœ… åˆå¹¶åçš„æ¨¡å‹å·²ä¿å­˜åˆ° {merged_dir}")
except Exception as e:
    print(f"   âŒ ä¿å­˜åˆå¹¶åçš„æ¨¡å‹å¤±è´¥: {e}")
    sys.exit(1)

# æ­¥éª¤ 3: å®‰è£… llama.cpp ç”¨äºè½¬æ¢
print("\nğŸ“¥ æ­¥éª¤ 3: æ­£åœ¨è®¾ç½® llama.cpp ç”¨äº GGUF è½¬æ¢...")

# å…‹éš† llama.cpp ä»“åº“
if not run_command(
    ["git", "clone", "https://github.com/ggerganov/llama.cpp.git", "/tmp/llama.cpp"],
    "æ­£åœ¨å…‹éš† llama.cpp ä»“åº“"
):
    print("   æ­£åœ¨å°è¯•å¤‡ç”¨å…‹éš†æ–¹æ³•...")
    # å°è¯•æµ…å…‹éš†ï¼ˆåªå…‹éš†æœ€æ–°ç‰ˆæœ¬ï¼Œä¸åŒ…å«å†å²è®°å½•ï¼‰
    if not run_command(
        ["git", "clone", "--depth", "1", "https://github.com/ggerganov/llama.cpp.git", "/tmp/llama.cpp"],
        "æ­£åœ¨å…‹éš† llama.cppï¼ˆæµ…å…‹éš†ï¼‰"
    ):
        sys.exit(1)

# å®‰è£… Python ä¾èµ–
print("   æ­£åœ¨å®‰è£… Python ä¾èµ–...")
if not run_command(
    ["pip", "install", "-r", "/tmp/llama.cpp/requirements.txt"],
    "æ­£åœ¨å®‰è£… llama.cpp ä¾èµ–"
):
    print("   âš ï¸  æŸäº›ä¾èµ–å¯èƒ½å·²ç»å®‰è£…")

if not run_command(
    ["pip", "install", "sentencepiece", "protobuf"],
    "æ­£åœ¨å®‰è£…åˆ†è¯å™¨ä¾èµ–"
):
    print("   âš ï¸  åˆ†è¯å™¨ä¾èµ–å¯èƒ½å·²ç»å®‰è£…")

# æ­¥éª¤ 4: è½¬æ¢ä¸º GGUFï¼ˆFP16 æ ¼å¼ï¼‰
print("\nğŸ”„ æ­¥éª¤ 4: æ­£åœ¨è½¬æ¢ä¸º GGUF æ ¼å¼ï¼ˆFP16ï¼‰...")
gguf_output_dir = "/tmp/gguf_output"  # GGUF è¾“å‡ºç›®å½•
os.makedirs(gguf_output_dir, exist_ok=True)  # åˆ›å»ºè¾“å‡ºç›®å½•ï¼Œå¦‚æœå·²å­˜åœ¨åˆ™ä¸æŠ¥é”™

convert_script = "/tmp/llama.cpp/convert_hf_to_gguf.py"  # è½¬æ¢è„šæœ¬è·¯å¾„
model_name = ADAPTER_MODEL.split('/')[-1]  # ä»æ¨¡å‹è·¯å¾„ä¸­æå–æ¨¡å‹åç§°
gguf_file = f"{gguf_output_dir}/{model_name}-f16.gguf"  # è¾“å‡ºæ–‡ä»¶è·¯å¾„

print(f"   æ­£åœ¨è¿è¡Œè½¬æ¢...")
if not run_command(
    [
        sys.executable, convert_script,  # ä½¿ç”¨å½“å‰ Python è§£é‡Šå™¨æ‰§è¡Œè½¬æ¢è„šæœ¬
        merged_dir,  # è¾“å…¥æ¨¡å‹ç›®å½•
        "--outfile", gguf_file,  # è¾“å‡ºæ–‡ä»¶è·¯å¾„
        "--outtype", "f16"  # è¾“å‡ºç±»å‹ä¸º FP16ï¼ˆåŠç²¾åº¦æµ®ç‚¹ï¼‰
    ],
    f"æ­£åœ¨è½¬æ¢ä¸º FP16"
):
    print("   âŒ è½¬æ¢å¤±è´¥ï¼")
    sys.exit(1)

print(f"   âœ… FP16 GGUF å·²åˆ›å»º: {gguf_file}")

# æ­¥éª¤ 5: é‡åŒ–ä¸ºä¸åŒæ ¼å¼
print("\nâš™ï¸  æ­¥éª¤ 5: æ­£åœ¨åˆ›å»ºé‡åŒ–ç‰ˆæœ¬...")

# ä½¿ç”¨ CMake æ„å»ºé‡åŒ–å·¥å…·ï¼ˆæ¯” make æ›´å¯é ï¼‰
print("   æ­£åœ¨ä½¿ç”¨ CMake æ„å»ºé‡åŒ–å·¥å…·...")
os.makedirs("/tmp/llama.cpp/build", exist_ok=True)  # åˆ›å»ºæ„å»ºç›®å½•

# ä½¿ç”¨ CMake é…ç½®
# -B æŒ‡å®šæ„å»ºç›®å½•ï¼Œ-S æŒ‡å®šæºä»£ç ç›®å½•
# -DGGML_CUDA=OFF ç¦ç”¨ CUDA æ”¯æŒï¼ˆå¦‚æœéœ€è¦ GPU åŠ é€Ÿå¯è®¾ç½®ä¸º ONï¼‰
if not run_command(
    ["cmake", "-B", "/tmp/llama.cpp/build", "-S", "/tmp/llama.cpp",
     "-DGGML_CUDA=OFF"],
    "æ­£åœ¨ä½¿ç”¨ CMake é…ç½®"
):
    print("   âŒ CMake é…ç½®å¤±è´¥")
    sys.exit(1)

# åªæ„å»ºé‡åŒ–å·¥å…·
# --target æŒ‡å®šæ„å»ºç›®æ ‡ï¼Œ-j 4 ä½¿ç”¨ 4 ä¸ªå¹¶è¡Œä»»åŠ¡
if not run_command(
    ["cmake", "--build", "/tmp/llama.cpp/build", "--target", "llama-quantize", "-j", "4"],
    "æ­£åœ¨æ„å»º llama-quantize"
):
    print("   âŒ æ„å»ºå¤±è´¥ï¼")
    sys.exit(1)

print("   âœ… é‡åŒ–å·¥å…·å·²æ„å»º")

# ä½¿ç”¨ CMake æ„å»ºè¾“å‡ºè·¯å¾„
quantize_bin = "/tmp/llama.cpp/build/bin/llama-quantize"  # é‡åŒ–å·¥å…·å¯æ‰§è¡Œæ–‡ä»¶

# å¸¸ç”¨é‡åŒ–æ ¼å¼
# æ¯ä¸ªå…ƒç»„åŒ…å«ï¼š(é‡åŒ–ç±»å‹, æè¿°)
quant_formats = [
    ("Q4_K_M", "4 ä½ï¼Œä¸­ç­‰è´¨é‡ï¼ˆæ¨èï¼‰"),
    ("Q5_K_M", "5 ä½ï¼Œæ›´é«˜è´¨é‡"),
    ("Q8_0", "8 ä½ï¼Œéå¸¸é«˜è´¨é‡"),
]

quantized_files = []  # å­˜å‚¨ç”Ÿæˆçš„é‡åŒ–æ–‡ä»¶åˆ—è¡¨
for quant_type, description in quant_formats:
    print(f"   æ­£åœ¨åˆ›å»º {quant_type} é‡åŒ–ï¼ˆ{description}ï¼‰...")
    quant_file = f"{gguf_output_dir}/{model_name}-{quant_type.lower()}.gguf"  # é‡åŒ–æ–‡ä»¶è·¯å¾„

    if not run_command(
        [quantize_bin, gguf_file, quant_file, quant_type],  # æ‰§è¡Œé‡åŒ–å‘½ä»¤
        f"æ­£åœ¨é‡åŒ–ä¸º {quant_type}"
    ):
        print(f"   âš ï¸  ç”±äºé”™è¯¯è·³è¿‡ {quant_type}")
        continue

    quantized_files.append((quant_file, quant_type))  # æ·»åŠ åˆ°å·²ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨

    # è·å–æ–‡ä»¶å¤§å°
    size_mb = os.path.getsize(quant_file) / (1024 * 1024)  # è½¬æ¢ä¸º MB
    print(f"   âœ… {quant_type}: {size_mb:.1f} MB")

if not quantized_files:
    print("   âŒ æ²¡æœ‰æˆåŠŸåˆ›å»ºä»»ä½•é‡åŒ–ç‰ˆæœ¬")
    sys.exit(1)

# æ­¥éª¤ 6: ä¸Šä¼ åˆ° Hub
print("\nâ˜ï¸  æ­¥éª¤ 6: æ­£åœ¨ä¸Šä¼ åˆ° Hugging Face Hub...")
api = HfApi()  # åˆ›å»º Hugging Face API å®ä¾‹

# åˆ›å»ºä»“åº“
print(f"   æ­£åœ¨åˆ›å»ºä»“åº“: {OUTPUT_REPO}")
try:
    # create_repo åˆ›å»ºæ–°ä»“åº“ï¼Œexist_ok=True å¦‚æœä»“åº“å·²å­˜åœ¨åˆ™ä¸æŠ¥é”™
    api.create_repo(repo_id=OUTPUT_REPO, repo_type="model", exist_ok=True)
    print("   âœ… ä»“åº“å·²å°±ç»ª")
except Exception as e:
    print(f"   â„¹ï¸  ä»“åº“å¯èƒ½å·²å­˜åœ¨: {e}")

# ä¸Šä¼  FP16 ç‰ˆæœ¬
print("   æ­£åœ¨ä¸Šä¼  FP16 GGUF...")
try:
    # upload_file ä¸Šä¼ æ–‡ä»¶åˆ°ä»“åº“
    api.upload_file(
        path_or_fileobj=gguf_file,  # æœ¬åœ°æ–‡ä»¶è·¯å¾„
        path_in_repo=f"{model_name}-f16.gguf",  # ä»“åº“ä¸­çš„æ–‡ä»¶å
        repo_id=OUTPUT_REPO,  # ä»“åº“ ID
    )
    print("   âœ… FP16 å·²ä¸Šä¼ ")
except Exception as e:
    print(f"   âŒ ä¸Šä¼ å¤±è´¥: {e}")
    sys.exit(1)

# ä¸Šä¼ é‡åŒ–ç‰ˆæœ¬
for quant_file, quant_type in quantized_files:
    print(f"   æ­£åœ¨ä¸Šä¼  {quant_type}...")
    try:
        api.upload_file(
            path_or_fileobj=quant_file,
            path_in_repo=f"{model_name}-{quant_type.lower()}.gguf",
            repo_id=OUTPUT_REPO,
        )
        print(f"   âœ… {quant_type} å·²ä¸Šä¼ ")
    except Exception as e:
        print(f"   âŒ {quant_type} ä¸Šä¼ å¤±è´¥: {e}")
        continue

# åˆ›å»º README
print("\nğŸ“ æ­£åœ¨åˆ›å»º README...")
readme_content = f"""---
base_model: {BASE_MODEL}
tags:
- gguf
- llama.cpp
- quantized
- trl
- sft
---

# {OUTPUT_REPO.split('/')[-1]}

è¿™æ˜¯ [{ADAPTER_MODEL}](https://huggingface.co/{ADAPTER_MODEL}) çš„ GGUF è½¬æ¢ç‰ˆæœ¬ï¼Œå®ƒæ˜¯ [{BASE_MODEL}](https://huggingface.co/{BASE_MODEL}) çš„ LoRA å¾®è°ƒç‰ˆæœ¬ã€‚

## æ¨¡å‹è¯¦æƒ…

- **åŸºç¡€æ¨¡å‹ï¼š** {BASE_MODEL}
- **å¾®è°ƒæ¨¡å‹ï¼š** {ADAPTER_MODEL}
- **è®­ç»ƒï¼š** ä½¿ç”¨ TRL è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰
- **æ ¼å¼ï¼š** GGUFï¼ˆç”¨äº llama.cppã€Ollamaã€LM Studio ç­‰ï¼‰

## å¯ç”¨é‡åŒ–ç‰ˆæœ¬

| æ–‡ä»¶ | é‡åŒ– | å¤§å° | æè¿° | ä½¿ç”¨åœºæ™¯ |
|------|-------|------|-------------|----------|
| {model_name}-f16.gguf | F16 | ~1GB | å…¨ç²¾åº¦ | æœ€ä½³è´¨é‡ï¼Œé€Ÿåº¦è¾ƒæ…¢ |
| {model_name}-q8_0.gguf | Q8_0 | ~500MB | 8 ä½ | é«˜è´¨é‡ |
| {model_name}-q5_k_m.gguf | Q5_K_M | ~350MB | 5 ä½ä¸­ç­‰ | è‰¯å¥½è´¨é‡ï¼Œæ›´å° |
| {model_name}-q4_k_m.gguf | Q4_K_M | ~300MB | 4 ä½ä¸­ç­‰ | æ¨è - è‰¯å¥½å¹³è¡¡ |

## ä½¿ç”¨æ–¹æ³•

### ä½¿ç”¨ llama.cpp

```bash
# ä¸‹è½½æ¨¡å‹
huggingface-cli download {OUTPUT_REPO} {model_name}-q4_k_m.gguf

# ä½¿ç”¨ llama.cpp è¿è¡Œ
./llama-cli -m {model_name}-q4_k_m.gguf -p "åœ¨æ­¤è¾“å…¥æ‚¨çš„æç¤ºè¯"
```

### ä½¿ç”¨ Ollama

1. åˆ›å»ºä¸€ä¸ª `Modelfile`ï¼š
```
FROM ./{model_name}-q4_k_m.gguf
```

2. åˆ›å»ºæ¨¡å‹ï¼š
```bash
ollama create my-model -f Modelfile
ollama run my-model
```

### ä½¿ç”¨ LM Studio

1. ä¸‹è½½ `.gguf` æ–‡ä»¶
2. å¯¼å…¥åˆ° LM Studio
3. å¼€å§‹èŠå¤©ï¼

## è®¸å¯è¯

ç»§æ‰¿è‡ªåŸºç¡€æ¨¡å‹çš„è®¸å¯è¯ï¼š{BASE_MODEL}

## å¼•ç”¨

```bibtex
@misc{{{OUTPUT_REPO.split('/')[-1].replace('-', '_')},
  author = {{{username}}},
  title = {{{OUTPUT_REPO.split('/')[-1]}}},
  year = {{2025}},
  publisher = {{Hugging Face}},
  url = {{https://huggingface.co/{OUTPUT_REPO}}}
}}
```

---

*ä½¿ç”¨ llama.cpp è½¬æ¢ä¸º GGUF æ ¼å¼*
"""

try:
    # ä¸Šä¼  README æ–‡ä»¶
    # encode() å°†å­—ç¬¦ä¸²ç¼–ç ä¸ºå­—èŠ‚
    api.upload_file(
        path_or_fileobj=readme_content.encode(),
        path_in_repo="README.md",
        repo_id=OUTPUT_REPO,
    )
    print("   âœ… README å·²ä¸Šä¼ ")
except Exception as e:
    print(f"   âŒ README ä¸Šä¼ å¤±è´¥: {e}")

print("\n" + "=" * 60)
print("âœ… GGUF è½¬æ¢å®Œæˆï¼")
print(f"ğŸ“¦ ä»“åº“ï¼šhttps://huggingface.co/{OUTPUT_REPO}")
print(f"\nğŸ“¥ ä¸‹è½½å‘½ä»¤ï¼š")
print(f"   huggingface-cli download {OUTPUT_REPO} {model_name}-q4_k_m.gguf")
print(f"\nğŸš€ ä½¿ç”¨ Ollamaï¼š")
print("   1. ä¸‹è½½ GGUF æ–‡ä»¶")
print(f"   2. åˆ›å»º Modelfile: FROM ./{model_name}-q4_k_m.gguf")
print("   3. ollama create my-model -f Modelfile")
print("   4. ollama run my-model")
print("=" * 60)
